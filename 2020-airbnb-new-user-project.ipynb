{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Filter warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# model lib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost.sklearn import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading dataset\n\nloading train, test and session data from zip"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/airbnb-recruiting-new-user-bookings/train_users_2.csv.zip')\ntest_data = pd.read_csv('../input/airbnb-recruiting-new-user-bookings/test_users.csv.zip')\nprint(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"session_data= pd.read_csv('../input/airbnb-recruiting-new-user-bookings/sessions.csv.zip')\nprint(session_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# see cols with missing value in train_data\nmissing_value_by_train_cols = [cols for cols in train_data.columns\n                         if train_data[cols].isnull().any()]\n\nmissing_ratio_in_train_data = ['{:0.1f}%'.format(100 * train_data[cols].isnull().sum() / train_data.shape[0])\n                               for cols in missing_value_by_train_cols]\n\n# see cols with missing value in test_data\nmissing_value_by_test_cols = [cols for cols in test_data.columns\n                         if test_data[cols].isnull().any()]\n\nmissing_ratio_in_test_data = ['{:0.1f}%'.format(100 * test_data[cols].isnull().sum() / test_data.shape[0])\n                               for cols in missing_value_by_test_cols]\n\nprint(missing_value_by_train_cols)\nprint(missing_ratio_in_train_data)\nprint(missing_value_by_test_cols)\nprint(missing_ratio_in_test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"session_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insight from session_data\n* If message_post in action_type and action_detail, then it will get NaN in action. But there is not always secs_elapsed.\n* If action is lookup, then action_type and action_detail will be NaN."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_action_feature_ratio(before_n_rank):\n    not_null_session_action = session_data[session_data['action'].notnull()]['action']\n    not_null_ratio = 100 * not_null_session_action.value_counts().sort_values(ascending=False)[:before_n_rank] / session_data.action.count()\n    not_null_ratio.hist(bins=50, density=False, cumulative=True)\n    return 'Ratio contains: {}'.format(not_null_ratio.sum())\n\nplot_action_feature_ratio(35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"session_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can find that \"date_first_booking\" does not exist in the train_data, so we can remove it from the data.\n* Age is missing about half of all data, so we need to fill missing value\n* the 'unknown' needs to change to np.nan in order to set as Nan."},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n\nWe simply use all the feature to set as baseline model. In this step, we will seperate the date into year, month and day for each column. and turn the categorical columns into one hot code."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new dataframe called 'df_total' as feature engineer matrix\ndf_train = train_data.drop(['country_destination'], axis=1)\ndf_total = pd.concat([df_train, test_data], axis=0, ignore_index=True)\ndf_total = df_total.drop(['date_first_booking'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total data for Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperate the date into three part\ndate_account_created = np.vstack(df_total['date_account_created'].astype(str).apply(lambda x: \n                                                                                    list(map(int, x.split('-')))))\ndf_total['date_account_created_year'] = date_account_created[:,0]\ndf_total['date_account_created_month'] = date_account_created[:,1]\n# df_total['date_account_created_day'] = date_account_created[:,2]\n\n# drop the date_account_create because it is no longer use\ndf_total = df_total.drop('date_account_created', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperate the date into six parts\nfirst_active = np.vstack(df_total['timestamp_first_active'].astype(str).apply(lambda x:\n                                                                              list(map(int,[x[:4], x[4:6], x[6:8]]))))\ndf_total['first_active_year'] = date_account_created[:,0]\ndf_total['first_active_month'] = date_account_created[:,1]\n# df_total['first_active_day'] = date_account_created[:,2]\n\n# drop the date_account_create because it is no longer use\ndf_total = df_total.drop('timestamp_first_active', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correct age with right type\ndf_total['age'] = df_total['age'].apply(lambda x: int(2015 - x) if x > 1750 else x)\ndf_total['age'] = df_total.age.apply(lambda x: np.log2(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total.age.hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill age with simputer\nfrom sklearn.impute import SimpleImputer\n\nage_freq_imputer = SimpleImputer(strategy='mean')\nage = df_total.loc[:,\"age\"].values.reshape(-1,1)\nage_freq_imputer.fit_transform(age)\ndf_total.loc[:,\"age\"] = age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Fill na in lang col\nlang_freq_imputer = SimpleImputer(strategy='most_frequent')\nlang = df_total.loc[:,\"language\"].values.reshape(-1,1)\nlang_freq_imputer.fit_transform(lang)\ndf_total.loc[:,\"language\"] = lang","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Session engineering\n* Filled NaN with 'NULL' as a missing value\n* calculate each user by\n    * number of actions taken\n    * number of unique action_type, action_details, device\n    * sum of seconds of elasped"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new session_data to execute data engineering\ndf_session = session_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have a peek with ratio of missing values in session_data\nprint('action ratio: {:0.2f}%'.format(session_data['action'].isnull().sum() / session_data['action'].count()))\nprint('action_type ratio: {:0.2f}%'.format(session_data['action_type'].isnull().sum() / session_data['action_type'].count()))\nprint('action_detail ratio: {:0.2f}%'.format(session_data['action_detail'].isnull().sum() / session_data['action_detail'].count()))\nprint('sec_elasped ratio: {:0.2f}%'.format(session_data['secs_elapsed'].isnull().sum() / session_data['secs_elapsed'].count()))\n\n# We simply fill na with 'NULL' as a new type and replace data which is '-unknown-' with 'NULL'\n# But replace secs as np.nan because we need to calculate sum secs after\ndf_session = df_session.replace('-unknown-', 'NULL')\ndf_session = df_session.fillna('NULL')\ndf_session['secs_elapsed'] = df_session['secs_elapsed'].replace('NULL', np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Peek all '-unknown-' is replaced by NaN successfully.\nassert df_session.where(df_session == '-unknown-') == 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate each user by unique action_type, action_details, device\naction_count = session_data.groupby('user_id')['action'].count().reset_index()\nunq_action_count = session_data.groupby('user_id')['action'].nunique().reset_index()\nunq_action_type = session_data.groupby('user_id')['action_type'].nunique().reset_index()\nunq_action_detail = session_data.groupby('user_id')['action_detail'].nunique().reset_index()\nunq_device = session_data.groupby('user_id')['device_type'].nunique().reset_index()\n\n# Calculate each usr by summarizing sec_elapsed\nsum_sec_elapsed = session_data.groupby('user_id')['secs_elapsed'].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename all new columns\naction_count.columns = ['user_id', 'action_count']\nunq_action_count.columns = ['user_id', 'unq_action_count']\nunq_action_type.columns = ['user_id', 'unq_action_type']\nunq_action_detail.columns = ['user_id', 'unq_action_detail']\nunq_device.columns = ['user_id', 'unq_device']\nsum_sec_elapsed.columns = ['user_id', 'sum_sec_elapsed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_sec_elapsed.hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# log tansformation\nlog_action_count = action_count.action_count.apply(lambda x: np.log(x + 1))\nlog_unq_action_count = unq_action_count.unq_action_count.apply(lambda x: np.log(x + 1))\nlog_unq_action_type = unq_action_type.unq_action_type.apply(lambda x: np.log(x + 1))\nlog_unq_action_detail = unq_action_detail.unq_action_detail.apply(lambda x: np.log(x + 1))\nlog_sum_sec_elapsed = sum_sec_elapsed.sum_sec_elapsed.apply(lambda x: np.log(x + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have a peek with data dist after log transformation\nplt.subplot(5,1,1)\nplt.hist(log_action_count, bins=100)\nplt.title('Distributions after Log trans')\nplt.subplot(5,1,2)\nplt.hist(log_unq_action_count, bins=100)\nplt.subplot(5,1,3)\nplt.hist(log_unq_action_type, bins=100)\nplt.subplot(5,1,4)\nplt.hist(log_unq_action_detail, bins=100)\nplt.subplot(5,1,5)\nplt.hist(log_sum_sec_elapsed, bins=100)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending back user_id to columns\nlog_action_count = pd.concat([action_count.user_id, log_action_count], axis=1)\nlog_unq_action_count = pd.concat([unq_action_count.user_id, log_unq_action_count], axis=1)\nlog_unq_action_type = pd.concat([unq_action_type.user_id, log_unq_action_type], axis=1)\nlog_unq_action_detail = pd.concat([unq_action_detail.user_id, log_unq_action_detail], axis=1)\nlog_sum_sec_elapsed = pd.concat([sum_sec_elapsed.user_id, log_sum_sec_elapsed], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging back to main dataframe\ndf_total = df_total.merge(log_action_count, left_on='id', right_on='user_id', how='left', suffixes=('', '_y'))\ndf_total = df_total.merge(log_unq_action_count, left_on='id', right_on='user_id', how='left', suffixes=('', '_y'))\ndf_total = df_total.merge(log_unq_action_type, left_on='id', right_on='user_id', how='left', suffixes=('', '_y'))\ndf_total = df_total.merge(log_unq_action_detail, left_on='id', right_on='user_id', how='left', suffixes=('', '_y'))\ndf_total = df_total.merge(unq_device, left_on='id', right_on='user_id', how='left', suffixes=('', '_y'))\ndf_total = df_total.merge(log_sum_sec_elapsed, left_on='id', right_on='user_id', how='left', suffixes=('', '_y'))\n\n# Drop duplicate calumns\ndf_total.drop(df_total.filter(regex='_y$').columns.tolist(),axis=1, inplace=True)\ndf_total.drop('user_id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fill NaN in affiliate tracked with most freq"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total.describe(include='object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_total[df_total.first_affiliate_tracked.isnull()]['first_affiliate_tracked']\ndf_total['first_affiliate_tracked'] = df_total['first_affiliate_tracked'].fillna('untracked')\n\n# Verify the fill na is successful\nassert df_total.first_affiliate_tracked.isnull().sum() == 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Impute missing value with KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Replacing unknown in gender with np.nan\ndf_total.gender.replace('-unknown-', np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a null df to store ohe\ndf_knn = pd.DataFrame()\n\nohe_to_fill_gender_na_feature = [\n    'gender',\n    'signup_method',\n    'signup_flow',\n    'language',\n    'affiliate_channel',\n    'affiliate_provider',\n    'first_affiliate_tracked',\n    'signup_app',\n    'first_device_type',\n    'first_browser'\n]\n\nfor feature in ohe_to_fill_gender_na_feature:\n    df_knn_le = pd.DataFrame(LabelEncoder().fit_transform(df_total[feature].apply(lambda x: str(x))))\n    df_knn = pd.concat([df_knn, df_knn_le], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute with KNN imputer\nKNN_imputer = KNNImputer()\nKNN_imputer = KNNImputer(n_neighbors=2,\n                         missing_values=np.nan,\n                         weights='uniform',\n                         metric='nan_euclidean',\n                         copy=True)\n\n# fill the na\nKNN_imputer.fit_transform(df_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute gender into df_total\ndf_total['gender'] = df_knn.loc[:,0]\n\nassert df_total.gender.isnull().sum() == 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_encoding_feature = [\n    'gender',\n    'signup_method',\n    'signup_flow',\n    'language',\n    'affiliate_channel',\n    'affiliate_provider',\n    'first_affiliate_tracked',\n    'signup_app',\n    'first_device_type',\n    'first_browser'\n]\n\nfor feature in one_hot_encoding_feature:\n    df_dummy = pd.get_dummies(df_total[feature], prefix=feature)\n    df_total.drop(feature, axis=1, inplace=True)\n    df_total = pd.concat([df_total, df_dummy], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split train and test to validate\nle = LabelEncoder()\n\ncountry_destination = train_data['country_destination']\ndf_final = df_total.drop('id', axis=1)\n\ntrain_y = le.fit_transform(country_destination)\ntrain_X = df_final.iloc[:train_data.shape[0],:]\ntest_X = df_final.iloc[train_data.shape[0]:, :]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing XGB Classifier\nxgb = XGBClassifier(max_depth=10,\n                    learning_rate=0.01,\n                    n_estimators=50,\n                    min_child_weight=1,\n                    objective='multi:softprob',\n                    subsample=0.5,\n                    colsample_bytree=0.5,\n                    seed=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_proba(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.classes_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verify the nCDG of Prediction\n\nHere is the function of nCDG function to calculate XGB performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import make_scorer, ndcg_score\nndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=5)\n\ndef dcg_score(y_true, y_score, k=5):\n    \"\"\"Discounted cumulative gain (DCG) at rank K.\n\n    Parameters\n    ----------\n    y_true : array, shape = [n_samples]\n        Ground truth (true relevance labels).\n    y_score : array, shape = [n_samples, n_classes]\n        Predicted scores.\n    k : int\n        Rank.\n\n    Returns\n    -------\n    score : float\n    \"\"\"\n    order = np.argsort(y_score)[::-1]\n    y_true = np.take(y_true, order[:k])\n\n    gain = 2 ** y_true - 1\n\n    discounts = np.log2(np.arange(len(y_true)) + 2)\n    return np.sum(gain / discounts)\n\n\ndef ndcg_score(ground_truth, predictions, k=5):\n    \"\"\"Normalized discounted cumulative gain (NDCG) at rank K.\n\n    Normalized Discounted Cumulative Gain (NDCG) measures the performance of a\n    recommendation system based on the graded relevance of the recommended\n    entities. It varies from 0.0 to 1.0, with 1.0 representing the ideal\n    ranking of the entities.\n\n    Parameters\n    ----------\n    ground_truth : array, shape = [n_samples]\n        Ground truth (true labels represended as integers).\n    predictions : array, shape = [n_samples, n_classes]\n        Predicted probabilities.\n    k : int\n        Rank.\n\n    Returns\n    -------\n    score : float\n\n    Example\n    -------\n    >>> ground_truth = [1, 0, 2]\n    >>> predictions = [[0.15, 0.55, 0.2], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]\n    >>> score = ndcg_score(ground_truth, predictions, k=2)\n    1.0\n    >>> predictions = [[0.9, 0.5, 0.8], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]\n    >>> score = ndcg_score(ground_truth, predictions, k=2)\n    0.6666666666\n    \"\"\"\n    lb = LabelBinarizer()\n    lb.fit(range(len(predictions) + 1))\n    T = lb.transform(ground_truth)\n\n    scores = []\n\n    # Iterate over each y_true and compute the DCG score\n    for y_true, y_score in zip(T, predictions):\n        actual = dcg_score(y_true, y_score, k)\n        best = dcg_score(y_true, y_true, k)\n        score = float(actual) / float(best)\n        scores.append(score)\n\n    return np.mean(scores)\n\n\n# NDCG Scorer function\nndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split train and test into 5 Kfold"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n\n# Initialized Kfold to use it later\nkf = KFold(n_splits=5, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\n    \ndef performance_measures(model, store_results=True):\n    train_ndcg = cross_val_score(model, X_train_transformed, y_train, scoring=ndcg_scorer, cv=kf, n_jobs=-1)\n    test_ndcg = cross_val_score(model, X_test_transformed, y_test, scoring=ndcg_scorer, cv=kf, n_jobs=-1)\n    print(\"Mean Train NDGC: {}\\nMean Test NDGC: {}\".format(train_ndcg.mean(), test_ndcg.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate the Final final to Submission.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize the data\ntest_id = []\ncities_list = []\n\nfor i in range(test_data.shape[0]):\n    each_id = [test_data['id'][i]]\n    test_id += each_id * 5\n    cities_list += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()\n\n# Make sure both columns have same rows\nprint('length of test_id: {}'.format(len(test_id)))\nprint('length of cities_list: {}'.format(len(cities_list)))\nassert len(test_id) == len(cities_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate submission.csv\nsub = pd.DataFrame(np.column_stack((test_id, cities_list)), columns=['id', 'country'])\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}